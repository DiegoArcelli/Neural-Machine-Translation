{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR1abG4foRZh"
      },
      "source": [
        "# Training prompt tuned T5 model for Neural Machine translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYGNqhd0oaBB"
      },
      "source": [
        "## Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBio5_qhyNP_",
        "outputId": "edee7aea-9c40-4ce7-91b9-10a925cc8a20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.9/dist-packages (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.10.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.13.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.4.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.10.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "48v-fNzeotlQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BartModel\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import random_split\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import evaluate\n",
        "from transformers import T5Model, BartModel, T5ForConditionalGeneration\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import T5Model\n",
        "from transformers.utils import logging\n",
        "logger = logging.get_logger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQW8BX1zo0i4"
      },
      "source": [
        "## Dowloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dFNc2Ego0nZ",
        "outputId": "8a16203e-412d-4e34-fc1d-f46d3f98f295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-26 13:12:35--  http://www.manythings.org/anki/ita-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7981351 (7.6M) [application/zip]\n",
            "Saving to: ‘ita-eng.zip’\n",
            "\n",
            "ita-eng.zip         100%[===================>]   7.61M  11.6MB/s    in 0.7s    \n",
            "\n",
            "2023-03-26 13:12:36 (11.6 MB/s) - ‘ita-eng.zip’ saved [7981351/7981351]\n",
            "\n",
            "Archive:  ita-eng.zip\n",
            "  inflating: ita.txt                 \n",
            "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: _about.txt              \n",
            "mkdir: cannot create directory ‘dataset’: File exists\n"
          ]
        }
      ],
      "source": [
        "!wget http://www.manythings.org/anki/ita-eng.zip\n",
        "!unzip ita-eng.zip\n",
        "!rm ita-eng.zip\n",
        "!mkdir dataset\n",
        "!mv ita.txt dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAr8NmF8xebO"
      },
      "source": [
        "## Defining some settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZDELeZbxekw",
        "outputId": "ffdb4327-244a-401b-fb93-89c789cd7486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘images’: File exists\n",
            "mkdir: cannot create directory ‘checkpoints’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir images\n",
        "!mkdir checkpoints\n",
        "\n",
        "DIR_PATH= \".\"\n",
        "DATASET_PATH = os.path.join(DIR_PATH, \"./dataset\")\n",
        "IMAGE_PATH = os.path.join(DIR_PATH, \"./images\")\n",
        "CHECKPOINT_DIR = os.path.join(DIR_PATH, \"./checkpoints\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKBOxh-6y_jE"
      },
      "source": [
        "## Defining utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6o5kEOGay_sP"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    n_params =  sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'The model has {n_params} trainable parameters')\n",
        "\n",
        "\n",
        "def plot_curves(curve_1, label_1, curve_2=None, label_2=None, fig_name=\"figure\", show=False):\n",
        "\n",
        "    plt.plot(curve_1, label = label_1)\n",
        "    if curve_2 is not None:\n",
        "        plt.plot(curve_2, label = label_2)\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{fig_name}\")\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "    plt.clf()\n",
        "\n",
        "    \n",
        "def plot_attention_mask(attention_mask, source_tokens, target_tokens):\n",
        "\n",
        "    skip_tokens = len(source_tokens) if \"[PAD]\" not in source_tokens else source_tokens.index(\"[PAD]\")\n",
        "    source_tokens = source_tokens[:skip_tokens]\n",
        "\n",
        "    attention_mask = attention_mask.squeeze(1)\n",
        "\n",
        "    attention_mask = attention_mask[:, :skip_tokens]\n",
        "\n",
        "    plt.xticks(ticks=[x for x in range(len(source_tokens))], labels=source_tokens, rotation=45)\n",
        "    plt.tick_params(top=True, labeltop=True, bottom=False, labelbottom=False)\n",
        "    plt.yticks(ticks=[x for x in range(len(target_tokens))], labels=target_tokens)\n",
        "    plt.imshow(attention_mask, cmap='gray', vmin=0, vmax=1)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brDtWVHLq4uZ"
      },
      "source": [
        "## Definition of the dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "fq1pfb9mq4Yv"
      },
      "outputs": [],
      "source": [
        "class AnkiDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path,\n",
        "                 tokenizer_src,\n",
        "                 tokenizer_dst,\n",
        "                 src_max_length,\n",
        "                 dst_max_length,\n",
        "                 subsample=False,\n",
        "                 frac=1.0,\n",
        "                 seed=42\n",
        "                ) -> None:\n",
        "        super().__init__()\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_dst = tokenizer_dst\n",
        "        self.src_max_length = src_max_length\n",
        "        self.dst_max_length = dst_max_length\n",
        "        self.seed = seed\n",
        "        self.frac = frac\n",
        "        self.subsample = subsample\n",
        "        random.seed(self.seed)\n",
        "        self.data = self.get_data(data_path)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        src, dst = self.data[index]\n",
        "\n",
        "        src = self.tokenizer_src(src, max_length=self.src_max_length, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "        dst = self.tokenizer_dst(dst, max_length=self.dst_max_length, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "            \n",
        "        for key in src.keys():\n",
        "            src[key] = src[key][0]\n",
        "            dst[key] = dst[key][0]\n",
        "\n",
        "        return (src, dst)\n",
        "        \n",
        "\n",
        "\n",
        "    '''\n",
        "    Takes in input the path of the datasets and it returnes a list where each element of\n",
        "    the list is a list of the elment containing the english and italian sentence\n",
        "    '''\n",
        "    def get_data(self, data_path=\"./../dataset/ita.txt\"):\n",
        "\n",
        "        with open(data_path, \"r\") as dataset:\n",
        "            sentences = [tuple(sentence.split(\"\\t\")[:2]) for sentence in dataset.readlines()]\n",
        "            \n",
        "        if self.subsample == True:\n",
        "            k = int(len(sentences)*self.frac)\n",
        "            sentences = random.sample(sentences, k)\n",
        "\n",
        "        return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAJ6ZeRsoqtR"
      },
      "source": [
        "## Code of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "f9dnnO3voMt1"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "super class that defines the behavior of the T5 model with the soft-prompts\n",
        "'''\n",
        "class T5PromptTuningMixin:\n",
        "\n",
        "    '''\n",
        "    wrapper of the from_pretrained class method to include the loading of the soft-prompts\n",
        "    '''\n",
        "    @classmethod\n",
        "    def from_pretrained(\n",
        "        cls,\n",
        "        pretrained_model_name_or_path,\n",
        "        encoder_soft_prompt_path = None,\n",
        "        decoder_soft_prompt_path = None,\n",
        "        encoder_n_tokens = None,\n",
        "        decoder_n_tokens = None,\n",
        "        encoder_hidden_dim = None,\n",
        "        decoder_hidden_dim = None,\n",
        "        initialize_from_vocab = True,\n",
        "        random_range = 0.5,\n",
        "        device=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \n",
        "        # getting the T5 model\n",
        "        model = super().from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
        "\n",
        "        # freezing all the parameters of the pretrained T5 model\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        '''\n",
        "        load the encoder soft prompts if the path is provided otheriwise they\n",
        "        are randomly initialized\n",
        "        '''\n",
        "        if encoder_soft_prompt_path is not None:\n",
        "            model.set_encoder_soft_prompts(encoder_soft_prompt_path)\n",
        "        else:\n",
        "            model.initialize_encoder_soft_prompts(encoder_n_tokens, encoder_hidden_dim, random_range)\n",
        "\n",
        "        '''\n",
        "        load the encoder soft prompts if the path is provided otheriwise they\n",
        "        are randomly initialized\n",
        "        '''\n",
        "        if decoder_soft_prompt_path is not None:\n",
        "            model.set_decoder_soft_prompts(decoder_soft_prompt_path)\n",
        "        else:\n",
        "            model.initialize_decoder_soft_prompts(decoder_n_tokens, decoder_hidden_dim, random_range)\n",
        "\n",
        "        model.encoder_n_tokens = encoder_n_tokens\n",
        "        model.decoder_n_tokens = decoder_n_tokens\n",
        "\n",
        "        enc_emb_size = model.encoder.get_input_embeddings().weight.shape[1]\n",
        "        dec_emb_size = model.decoder.get_input_embeddings().weight.shape[1]\n",
        "\n",
        "        encoder_emb_generator = nn.Sequential(\n",
        "            nn.Linear(encoder_hidden_dim, encoder_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoder_hidden_dim, enc_emb_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        model.encoder_emb_generator = encoder_emb_generator\n",
        "\n",
        "        decoder_emb_generator = nn.Sequential(\n",
        "            nn.Linear(decoder_hidden_dim, decoder_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(decoder_hidden_dim, dec_emb_size),\n",
        "            nn.Tanh()\n",
        "        ) \n",
        "\n",
        "        model.decoder_emb_generator = decoder_emb_generator\n",
        "\n",
        "        model.encoder_input_tokens = torch.arange(encoder_n_tokens).long().to(device)\n",
        "        model.decoder_input_tokens = torch.arange(decoder_n_tokens).long().to(device)\n",
        "\n",
        "        return model\n",
        "    \n",
        "\n",
        "    def initialize_encoder_soft_prompts(self, n_tokens, hidden_dim, random_range=0.5):\n",
        "        self.n_tokens = n_tokens\n",
        "        self.encoder_soft_prompt = nn.Embedding(n_tokens, hidden_dim)\n",
        "        # init_prompt_value = torch.FloatTensor(2, 10).uniform_(-random_range, random_range)\n",
        "        # self.encoder_soft_prompt.weight = nn.parameter.Parameter(init_prompt_value)\n",
        "\n",
        "\n",
        "    def set_encoder_soft_prompts(self, soft_prompt_path):\n",
        "        self.encoder_soft_prompt = torch.load(soft_prompt_path, map_location=torch.device(\"cpu\"))\n",
        "        self.n_tokens = self.encoder_soft_prompt.num_embeddings\n",
        "\n",
        "\n",
        "    def initialize_decoder_soft_prompts(self, n_tokens, hidden_dim, random_range=0.5):\n",
        "        self.n_tokens = n_tokens\n",
        "        self.decoder_soft_prompt = nn.Embedding(n_tokens, hidden_dim)\n",
        "        # init_prompt_value = torch.FloatTensor(2, 10).uniform_(-random_range, random_range)\n",
        "        # self.decoder_soft_prompt.weight = nn.parameter.Parameter(init_prompt_value)\n",
        "\n",
        "\n",
        "    def set_decoder_soft_prompts(self, soft_prompt_path):\n",
        "        self.decoder_soft_prompt = torch.load(soft_prompt_path, map_location=torch.device(\"cpu\"))\n",
        "        self.n_tokens = self.decoder_soft_prompt.num_embeddings\n",
        "\n",
        "\n",
        "    def concatenate_encoder_soft_prompts(self, input_ids):\n",
        "        inputs_emb = self.encoder_soft_prompt(self.encoder_input_tokens)\n",
        "        soft_prompts = self.encoder_emb_generator(inputs_emb)\n",
        "\n",
        "        embeddings = self.encoder.embed_tokens(input_ids)\n",
        "\n",
        "        soft_prompts = soft_prompts.repeat(embeddings.size(0), 1, 1)\n",
        "\n",
        "        inputs_concat = torch.cat([soft_prompts, embeddings], dim=1)\n",
        "        return inputs_concat\n",
        "    \n",
        "\n",
        "    def concatenate_decoder_soft_prompts(self, input_ids):\n",
        "        inputs_emb = self.decoder_soft_prompt(self.decoder_input_tokens)\n",
        "        soft_prompts = self.decoder_emb_generator(inputs_emb)\n",
        "        \n",
        "        embeddings = self.decoder.embed_tokens(input_ids)\n",
        "\n",
        "        soft_prompts = soft_prompts.repeat(embeddings.size(0), 1, 1)\n",
        "\n",
        "        inputs_concat = torch.cat([soft_prompts, embeddings], dim=1)\n",
        "        return inputs_concat\n",
        "\n",
        "\n",
        "    def extend_attention_mask(self, attention_mask):\n",
        "        batch_size = attention_mask.shape[0]\n",
        "        soft_prompts_mask = torch.full((batch_size, self.n_tokens), 1, dtype=torch.long)\n",
        "        extended_mask = torch.concat([soft_prompts_mask, attention_mask], dim=1)\n",
        "        return extended_mask\n",
        "    \n",
        "\n",
        "    def extend_labels(self, labels, ignore_index=-100):\n",
        "        batch_size = labels.shape[0]\n",
        "        soft_prompts_indices = torch.full((batch_size, self.decoder_n_tokens), ignore_index).to(self.device)\n",
        "        extended_labels = torch.concat([soft_prompts_indices, labels], dim=1)\n",
        "        return extended_labels\n",
        "\n",
        "\n",
        "    '''\n",
        "    forward pass of the T5 prompt tuning model\n",
        "\n",
        "    Input (only the relevants):\n",
        "    - input_ids: the inputs tokens of the encoder (batch_size, src_len)\n",
        "    - attention_mask: the attention mask of the encoder (batch_size, src_len)\n",
        "    - decoder_input_ids: the inputs tokens of the decoder (batch_size, dst_len)\n",
        "    - decoder_attention_mask: the attention mask of the decoder (batch_size, dst_len)\n",
        "\n",
        "    Output:\n",
        "    - logits: \n",
        "    - encoder_last_hidden_state: \n",
        "    '''\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        past_key_values=None,\n",
        "        attention_mask=None,\n",
        "        decoder_attention_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        encoder_outputs=None,\n",
        "        use_cache=None,\n",
        "        labels=None,\n",
        "        return_dict=None,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \n",
        "        if input_ids is not None:\n",
        "            '''\n",
        "            if input_ids are passed their embedding is concatenated to the\n",
        "            encoder soft prompts to generate input_embeds, a tensor\n",
        "            of size (batch_size, enc_n_tokens + seq_len, enc_hidden_dim)\n",
        "            '''\n",
        "            inputs_embeds = self.concatenate_encoder_soft_prompts(input_ids)\n",
        "            input_ids = None\n",
        "\n",
        "        if decoder_input_ids is not None:\n",
        "            '''\n",
        "            if decoder_input_ids are passed thier embedding is concatenated to the\n",
        "            decoder soft prompts to generate decoder_input_embeds, a tensor\n",
        "            of size (batch_size, dec_n_tokens + dst_len, dec_hidden_dim)\n",
        "            '''\n",
        "            decoder_inputs_embeds = self.concatenate_decoder_soft_prompts(decoder_input_ids)\n",
        "            decoder_input_ids = None\n",
        "\n",
        "        if attention_mask is not None and inputs_embeds is not None:\n",
        "            '''\n",
        "            if attention_mask is passed it is extended to include also the encoder\n",
        "            soft prompts, generating a tensor of size (batch_size, enc_n_tokens + seq_len)\n",
        "            '''\n",
        "            attention_mask = self.extend_attention_mask(attention_mask)\n",
        "\n",
        "        if decoder_attention_mask is not None:\n",
        "            '''\n",
        "            if decoder_attention_mask is passed it is extended to include also the decoder\n",
        "            soft prompts, generating a tensor of size (batch_size, dec_n_tokens + dst_len)\n",
        "            '''\n",
        "            decoder_attention_mask = self.extend_attention_mask(decoder_attention_mask)\n",
        "\n",
        "\n",
        "        if labels is not None:\n",
        "            '''\n",
        "            if labels is passed then it is extended to include the also the embeddings\n",
        "            '''\n",
        "            labels = self.extend_labels(labels)\n",
        "            \n",
        "        '''\n",
        "        we pass the encoder and decoder embeddings to the forward layer of T5\n",
        "        '''\n",
        "        return super().forward(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            labels=labels,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            return_dict=return_dict,\n",
        "            *args,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "\n",
        "        kwargs['inputs_embeds'] = self.concatenate_encoder_soft_prompts(kwargs['input_ids']).to(self.device)\n",
        "        kwargs['attention_mask']=self.extend_attention_mask(torch.ones([1,kwargs['inputs_embeds'].shape[1]-self.n_tokens]).long()).to(self.device)\n",
        "\n",
        "        del kwargs['input_ids']\n",
        "\n",
        "        return super().generate(*args, **kwargs)\n",
        "\n",
        "'''\n",
        "Defining the T5 model with prompt tuning superclassing T5PromptTuningUtils and \n",
        "T5ForConditionalGeneration (which adds the head for producing the logits)\n",
        "'''\n",
        "class T5PromptTuning(T5PromptTuningMixin, T5ForConditionalGeneration):\n",
        "\n",
        "    def __init__(self, config) -> None:\n",
        "        super(T5PromptTuning, self).__init__(config)\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "super class that defines the behavior of the T5 model with the soft-prompts\n",
        "'''\n",
        "class T5PromptTuningMixinSimple:\n",
        "\n",
        "    '''\n",
        "    wrapper of the from_pretrained class method to include the loading of the soft-prompts\n",
        "    '''\n",
        "    @classmethod\n",
        "    def from_pretrained(\n",
        "        cls,\n",
        "        pretrained_model_name_or_path,\n",
        "        encoder_soft_prompt_path = None,\n",
        "        decoder_soft_prompt_path = None,\n",
        "        encoder_n_tokens = None,\n",
        "        decoder_n_tokens = None,\n",
        "        initialize_from_vocab = True,\n",
        "        random_range = 0.5,\n",
        "        device=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \n",
        "        # getting the T5 model\n",
        "        model = super().from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
        "\n",
        "        # freezing all the parameters of the pretrained T5 model\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        '''\n",
        "        load the encoder soft prompts if the path is provided otheriwise they\n",
        "        are randomly initialized\n",
        "        '''\n",
        "        if encoder_soft_prompt_path is not None:\n",
        "            model.set_encoder_soft_prompts(encoder_soft_prompt_path)\n",
        "        else:\n",
        "            model.initialize_encoder_soft_prompts(encoder_n_tokens, random_range)\n",
        "\n",
        "        '''\n",
        "        load the encoder soft prompts if the path is provided otheriwise they\n",
        "        are randomly initialized\n",
        "        '''\n",
        "        if decoder_soft_prompt_path is not None:\n",
        "            model.set_decoder_soft_prompts(decoder_soft_prompt_path)\n",
        "        else:\n",
        "            model.initialize_decoder_soft_prompts(decoder_n_tokens, random_range)\n",
        "\n",
        "        model.encoder_n_tokens = encoder_n_tokens\n",
        "        model.decoder_n_tokens = decoder_n_tokens \n",
        "\n",
        "        model.encoder_input_tokens = torch.arange(encoder_n_tokens).long().to(device)\n",
        "        model.decoder_input_tokens = torch.arange(decoder_n_tokens).long().to(device)\n",
        "\n",
        "        return model\n",
        "    \n",
        "\n",
        "    def initialize_encoder_soft_prompts(self, n_tokens, random_range=0.5):\n",
        "        self.n_tokens = n_tokens\n",
        "        self.encoder_soft_prompt = nn.Embedding(n_tokens, self.config.d_model)\n",
        "        # init_prompt_value = torch.FloatTensor(2, 10).uniform_(-random_range, random_range)\n",
        "        # self.encoder_soft_prompt.weight = nn.parameter.Parameter(init_prompt_value)\n",
        "\n",
        "\n",
        "    def set_encoder_soft_prompts(self, soft_prompt_path):\n",
        "        self.encoder_soft_prompt = torch.load(soft_prompt_path, map_location=torch.device(\"cpu\"))\n",
        "        self.n_tokens = self.encoder_soft_prompt.num_embeddings\n",
        "\n",
        "\n",
        "    def initialize_decoder_soft_prompts(self, n_tokens, random_range=0.5):\n",
        "        self.n_tokens = n_tokens\n",
        "        self.decoder_soft_prompt = nn.Embedding(n_tokens, self.config.d_model)\n",
        "        # init_prompt_value = torch.FloatTensor(2, 10).uniform_(-random_range, random_range)\n",
        "        # self.decoder_soft_prompt.weight = nn.parameter.Parameter(init_prompt_value)\n",
        "\n",
        "\n",
        "    def set_decoder_soft_prompts(self, soft_prompt_path):\n",
        "        self.decoder_soft_prompt = torch.load(soft_prompt_path, map_location=torch.device(\"cpu\"))\n",
        "        self.n_tokens = self.decoder_soft_prompt.num_embeddings\n",
        "\n",
        "\n",
        "    def concatenate_encoder_soft_prompts(self, input_ids):\n",
        "        soft_prompts = self.encoder_soft_prompt(self.encoder_input_tokens)\n",
        "        embeddings = self.encoder.embed_tokens(input_ids)\n",
        "        soft_prompts = soft_prompts.repeat(embeddings.size(0), 1, 1)\n",
        "\n",
        "        inputs_concat = torch.cat([soft_prompts, embeddings], dim=1)\n",
        "        return inputs_concat\n",
        "    \n",
        "\n",
        "    def concatenate_decoder_soft_prompts(self, input_ids):\n",
        "        soft_prompts = self.decoder_soft_prompt(self.decoder_input_tokens)\n",
        "        embeddings = self.decoder.embed_tokens(input_ids)\n",
        "\n",
        "        soft_prompts = soft_prompts.repeat(embeddings.size(0), 1, 1)\n",
        "\n",
        "        inputs_concat = torch.cat([soft_prompts, embeddings], dim=1)\n",
        "        return inputs_concat\n",
        "\n",
        "\n",
        "    def extend_attention_mask(self, attention_mask):\n",
        "        batch_size = attention_mask.shape[0]\n",
        "        soft_prompts_mask = torch.full((batch_size, self.n_tokens), 1, dtype=torch.long)\n",
        "        extended_mask = torch.concat([soft_prompts_mask, attention_mask], dim=1)\n",
        "        return extended_mask\n",
        "    \n",
        "\n",
        "    def extend_labels(self, labels, ignore_index=-100):\n",
        "        batch_size = labels.shape[0]\n",
        "        soft_prompts_indices = torch.full((batch_size, self.decoder_n_tokens), ignore_index).to(self.device)\n",
        "        extended_labels = torch.concat([soft_prompts_indices, labels], dim=1)\n",
        "        return extended_labels\n",
        "\n",
        "\n",
        "    '''\n",
        "    forward pass of the T5 prompt tuning model\n",
        "\n",
        "    Input (only the relevants):\n",
        "    - input_ids: the inputs tokens of the encoder (batch_size, src_len)\n",
        "    - attention_mask: the attention mask of the encoder (batch_size, src_len)\n",
        "    - decoder_input_ids: the inputs tokens of the decoder (batch_size, dst_len)\n",
        "    - decoder_attention_mask: the attention mask of the decoder (batch_size, dst_len)\n",
        "\n",
        "    Output:\n",
        "    - logits: \n",
        "    - encoder_last_hidden_state: \n",
        "    '''\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        past_key_values=None,\n",
        "        attention_mask=None,\n",
        "        decoder_attention_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        encoder_outputs=None,\n",
        "        use_cache=None,\n",
        "        labels=None,\n",
        "        return_dict=None,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \n",
        "        if input_ids is not None:\n",
        "            '''\n",
        "            if input_ids are passed their embedding is concatenated to the\n",
        "            encoder soft prompts to generate input_embeds, a tensor\n",
        "            of size (batch_size, enc_n_tokens + seq_len, enc_hidden_dim)\n",
        "            '''\n",
        "            inputs_embeds = self.concatenate_encoder_soft_prompts(input_ids)\n",
        "            input_ids = None\n",
        "\n",
        "        if decoder_input_ids is not None:\n",
        "            '''\n",
        "            if decoder_input_ids are passed thier embedding is concatenated to the\n",
        "            decoder soft prompts to generate decoder_input_embeds, a tensor\n",
        "            of size (batch_size, dec_n_tokens + dst_len, dec_hidden_dim)\n",
        "            '''\n",
        "            decoder_inputs_embeds = self.concatenate_decoder_soft_prompts(decoder_input_ids)\n",
        "            decoder_input_ids = None\n",
        "\n",
        "        if attention_mask is not None and inputs_embeds is not None:\n",
        "            '''\n",
        "            if attention_mask is passed it is extended to include also the encoder\n",
        "            soft prompts, generating a tensor of size (batch_size, enc_n_tokens + seq_len)\n",
        "            '''\n",
        "            attention_mask = self.extend_attention_mask(attention_mask)\n",
        "\n",
        "        if decoder_attention_mask is not None:\n",
        "            '''\n",
        "            if decoder_attention_mask is passed it is extended to include also the decoder\n",
        "            soft prompts, generating a tensor of size (batch_size, dec_n_tokens + dst_len)\n",
        "            '''\n",
        "            decoder_attention_mask = self.extend_attention_mask(decoder_attention_mask)\n",
        "\n",
        "\n",
        "        if labels is not None:\n",
        "            '''\n",
        "            if labels is passed then it is extended to include the also the embeddings\n",
        "            '''\n",
        "            labels = self.extend_labels(labels)\n",
        "            \n",
        "        '''\n",
        "        we pass the encoder and decoder embeddings to the forward layer of T5\n",
        "        '''\n",
        "        return super().forward(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            labels=labels,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            return_dict=return_dict,\n",
        "            *args,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "\n",
        "        kwargs['inputs_embeds'] = self.concatenate_encoder_soft_prompts(kwargs['input_ids']).to(self.device)\n",
        "        kwargs['attention_mask']=self.extend_attention_mask(torch.ones([1,kwargs['inputs_embeds'].shape[1]-self.n_tokens]).long()).to(self.device)\n",
        "\n",
        "        del kwargs['input_ids']\n",
        "\n",
        "        return super().generate(*args, **kwargs)\n",
        "\n",
        "'''\n",
        "Defining the T5 model with prompt tuning superclassing T5PromptTuningUtils and \n",
        "T5ForConditionalGeneration (which adds the head for producing the logits)\n",
        "'''\n",
        "class T5PromptTuningSimple(T5PromptTuningMixinSimple, T5ForConditionalGeneration):\n",
        "\n",
        "    def __init__(self, config) -> None:\n",
        "        super(T5PromptTuningSimple, self).__init__(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_cYnQOFsBrr"
      },
      "source": [
        "## Defining the trainer superclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "UbYnUabcsBv7"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, model, src_tokenizer, dst_tokenizer, config) -> None:\n",
        "\n",
        "        self.device = config[\"device\"]\n",
        "        self.model = model.to(self.device)\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.dst_tokenizer = dst_tokenizer\n",
        "        self.config = config\n",
        "\n",
        "        pad_token = dst_tokenizer.pad_token\n",
        "        pad_token_idx = dst_tokenizer.convert_tokens_to_ids([pad_token])[0]\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=pad_token_idx)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.1)\n",
        "        self.pad_token_idx = pad_token_idx\n",
        "\n",
        "        self.metric = evaluate.load(\"bleu\")\n",
        "\n",
        "        if \"model_name\" in config:\n",
        "            self.model_name = config[\"model_name\"]\n",
        "        else:\n",
        "            self.model_name = self.model.__class__.__name__.lower()\n",
        "\n",
        "\n",
        "    \n",
        "    def set_seeds(self, seed):\n",
        "        torch.manual_seed(seed)\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "\n",
        "    def get_data_loader(self, batch_size, val_split=0.2, test_split=0.1):\n",
        "        \n",
        "        data_set = AnkiDataset(\n",
        "            f\"{DATASET_PATH}/ita.txt\",\n",
        "            self.src_tokenizer,\n",
        "            self.dst_tokenizer,\n",
        "            self.config[\"src_max_length\"],\n",
        "            self.config[\"dst_max_length\"]\n",
        "        )\n",
        "\n",
        "\n",
        "        n = len(data_set)\n",
        "\n",
        "        val_size = int(n*val_split)\n",
        "        test_size = int(n*test_split)\n",
        "        train_size = n - val_size - test_size\n",
        "\n",
        "\n",
        "        train_set, val_set, test_set = random_split(data_set, [train_size, val_size, test_size])\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "                    train_set,\n",
        "                    batch_size = batch_size\n",
        "                )\n",
        "        \n",
        "        val_loader = DataLoader(\n",
        "                    val_set,\n",
        "                    batch_size=batch_size\n",
        "                )\n",
        "        \n",
        "        test_loader = DataLoader(\n",
        "                    test_set,\n",
        "                    batch_size = batch_size\n",
        "                )\n",
        "        \n",
        "        return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "    def generate_learning_curvers(self, train_losses, val_losses):\n",
        "\n",
        "        plot_curves(\n",
        "            curve_1=train_losses,\n",
        "            curve_2=val_losses,\n",
        "            label_1=\"Train loss\",\n",
        "            label_2=\"Validation loss\",\n",
        "            fig_name=f\"{IMAGE_PATH}/loss_model_{self.model_name}\"\n",
        "        )\n",
        "\n",
        "        plot_curves(\n",
        "            curve_1=train_losses[:self.best_epoch],\n",
        "            curve_2=val_losses[:self.best_epoch],\n",
        "            label_1=\"Train loss\",\n",
        "            label_2=\"Validation loss\",\n",
        "            fig_name=f\"{IMAGE_PATH}/best_loss_model_{self.model_name}\"\n",
        "        )\n",
        "\n",
        "        plot_curves(\n",
        "            curve_1=train_losses,\n",
        "            label_1=\"Train loss\",\n",
        "            fig_name=f\"{IMAGE_PATH}/train_loss_model_{self.model_name}\"\n",
        "        )\n",
        "\n",
        "        plot_curves(\n",
        "            curve_1=train_losses[:self.best_epoch],\n",
        "            label_1=\"Train loss\",\n",
        "            fig_name=f\"{IMAGE_PATH}/best_train_loss_model_{self.model_name}\"\n",
        "        )\n",
        "\n",
        "        plot_curves(\n",
        "            curve_1=val_losses,\n",
        "            label_1=\"Val loss\",\n",
        "            fig_name=f\"{IMAGE_PATH}/val_loss_model_{self.model_name}\"\n",
        "        )\n",
        "\n",
        "        plot_curves(\n",
        "            curve_1=val_losses[:self.best_epoch],\n",
        "            label_1=\"Val loss\",\n",
        "            fig_name=f\"{IMAGE_PATH}/best_val_loss_model_{self.model_name}\"\n",
        "        )\n",
        "\n",
        "\n",
        "    def train(self, generate_fun):\n",
        "        \n",
        "        seed = self.config[\"seed\"]\n",
        "        self.set_seeds(seed)\n",
        "\n",
        "        batch_size = self.config[\"batch_size\"]\n",
        "        # self.model.to(self.device)\n",
        "\n",
        "        train_loader, val_loader, test_loader = self.get_data_loader(batch_size, 0.2, 0.1)\n",
        "\n",
        "        self.train_loop(train_loader, val_loader)\n",
        "        self.model.eval()\n",
        "        test_loss = self.test_step(test_loader)\n",
        "        print(\"Evaluating model on the test set\")\n",
        "        print(f\"Test loss: {test_loss}\")\n",
        "\n",
        "        # evaluate bleu score\n",
        "        train_score = self.metric_evaluation(train_loader, generate_fun)\n",
        "        val_score = self.metric_evaluation(val_loader, generate_fun)\n",
        "        test_score = self.metric_evaluation(test_loader, generate_fun)\n",
        "\n",
        "        print(f\"Average train set BLEU score: {train_score}\")\n",
        "        print(f\"Average validation set BLEU score: {val_score}\")\n",
        "        print(f\"Average test set BLEU score: {test_score}\")\n",
        "\n",
        "\n",
        "    def train_loop(self, train_loader, val_loader):\n",
        "\n",
        "        epochs = self.config[\"max_epochs\"]\n",
        "        batch_size = self.config[\"batch_size\"]\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_loss_epoch = None\n",
        "\n",
        "        for epoch in range(1, epochs+1):\n",
        "            self.model.train()\n",
        "            print(f\"Training epoch {epoch}/{epochs}\")\n",
        "            train_loss = self.train_step(train_loader, epoch)\n",
        "            self.model.eval()\n",
        "            print(f\"Validation epoch {epoch}/{epochs}\")\n",
        "            val_loss = self.val_step(val_loader, epoch)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                if best_loss_epoch != None:\n",
        "                    os.system(f\"rm {CHECKPOINT_DIR}/model_{self.model_name}_{best_loss_epoch}_checkpoint.pt\")\n",
        "                best_val_loss = val_loss\n",
        "                best_loss_epoch = epoch\n",
        "                torch.save(self.model.state_dict(), f\"{CHECKPOINT_DIR}/model_{self.model_name}_{epoch}_checkpoint.pt\")\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch} train loss: {train_loss}, val_loss: {val_loss}\")\n",
        "\n",
        "        self.best_epoch = best_loss_epoch\n",
        "\n",
        "        self.generate_learning_curvers(train_losses, val_losses)\n",
        "        \n",
        "\n",
        "\n",
        "    def train_step(self, train_loader, epoch):\n",
        "\n",
        "        total_loss = 0\n",
        "        n = len(train_loader)\n",
        "\n",
        "        with tqdm(total=n) as pbar:\n",
        "            for step, batch in enumerate(train_loader):\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                inputs, targets = batch\n",
        "\n",
        "                inputs = inputs.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                input_ids = inputs.input_ids\n",
        "                target_ids = targets.input_ids\n",
        "\n",
        "                output = self.model(input_ids=input_ids, decoder_input_ids=target_ids)\n",
        "\n",
        "                logits = output.logits\n",
        "\n",
        "                logits_dim = logits.shape[-1]\n",
        "\n",
        "                logits = logits[1:].view(-1, logits_dim)\n",
        "                target_ids = target_ids[1:].reshape(-1)\n",
        "\n",
        "                loss = self.criterion(logits, target_ids)\n",
        "                \n",
        "                loss.backward()\n",
        "\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                if (step+1) % 50 == 0:\n",
        "                    print(f\"\\nEpoch {epoch}, samples {step+1}/{n} train loss: {total_loss/(step+1)}\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "                \n",
        "        avg_loss = total_loss / n\n",
        "\n",
        "        return avg_loss\n",
        "            \n",
        "    \n",
        "    def val_step(self, val_loader, epoch):\n",
        "\n",
        "        total_loss = 0\n",
        "        n = len(val_loader)\n",
        "\n",
        "        with tqdm(total=n) as pbar:\n",
        "            for step, batch in enumerate(val_loader):\n",
        "\n",
        "                inputs, targets = batch\n",
        "\n",
        "                inputs = inputs.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                input_ids = inputs.input_ids\n",
        "                target_ids = targets.input_ids\n",
        "\n",
        "                output = self.model(input_ids=input_ids, decoder_input_ids=target_ids)\n",
        "                logits = output.logits\n",
        "\n",
        "                logits_dim = logits.shape[-1]\n",
        "\n",
        "                logits = logits[1:].view(-1, logits_dim)\n",
        "                target_ids = target_ids[1:].reshape(-1)\n",
        "\n",
        "                loss = self.criterion(logits, target_ids)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                if (step+1) % 50 == 0:\n",
        "                    print(f\"\\nEpoch {epoch}, samples {step+1}/{n} train loss: {total_loss/(step+1)}\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "        avg_loss = total_loss / n\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "\n",
        "    \n",
        "    def test_step(self, test_loader):\n",
        "\n",
        "        self.model.load_state_dict(torch.load(f\"{CHECKPOINT_DIR}/model_{self.model_name}_{self.best_epoch}_checkpoint.pt\"))\n",
        "        \n",
        "        total_loss = 0\n",
        "        n = len(test_loader)\n",
        "\n",
        "        with tqdm(total=n) as pbar:\n",
        "            for step, batch in enumerate(test_loader):\n",
        "                \n",
        "                inputs, targets = batch\n",
        "\n",
        "                inputs = inputs.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                input_ids = inputs.input_ids\n",
        "                target_ids = targets.input_ids\n",
        "\n",
        "                output = self.model(input_ids=input_ids, decoder_input_ids=target_ids)\n",
        "                logits = output.logits\n",
        "\n",
        "                logits_dim = logits.shape[-1]\n",
        "\n",
        "                logits = logits[1:].view(-1, logits_dim)\n",
        "                target_ids = target_ids[1:].reshape(-1)\n",
        "\n",
        "                loss = self.criterion(logits, target_ids)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "        avg_loss = total_loss / n\n",
        "\n",
        "        return avg_loss\n",
        "            \n",
        "\n",
        "    \n",
        "\n",
        "    def metric_evaluation(self, data_loader, generate_fun):\n",
        "        \n",
        "        self.model.load_state_dict(torch.load(f\"{CHECKPOINT_DIR}/model_{self.model_name}_{self.best_epoch}_checkpoint.pt\"))\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "        score = 0\n",
        "\n",
        "        for step, batch in enumerate(data_loader):\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            inputs, targets = batch\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            for i in range(len(inputs.input_ids)):\n",
        "\n",
        "                input_ids = inputs.input_ids[i]\n",
        "                target_ids = targets.input_ids[i]\n",
        "\n",
        "                output = generate_fun(input_ids.unsqueeze(0))\n",
        "\n",
        "                if type(output) == tuple:\n",
        "                    pred_ids, attention = output\n",
        "                else:\n",
        "                    pred_ids = output[0]\n",
        "\n",
        "                pred_sentence = self.src_tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
        "                target_sentence = self.dst_tokenizer.decode(target_ids, skip_special_tokens=True)\n",
        "\n",
        "                result = self.metric.compute(predictions=[pred_sentence], references=[target_sentence])\n",
        "                score += result[\"bleu\"]\n",
        "\n",
        "            score /= len(data_loader)\n",
        "\n",
        "            return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwU55Kb1gG0J"
      },
      "source": [
        "## Prompt Tuning Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "EcGgRfE-gG0J"
      },
      "outputs": [],
      "source": [
        "class PromptTuningTrainer(Trainer):\n",
        "\n",
        "\n",
        "    def __init__(self, model, src_tokenizer, dst_tokenizer, config) -> None:\n",
        "        super(PromptTuningTrainer, self).__init__(model, src_tokenizer, dst_tokenizer, config)\n",
        "\n",
        "    \n",
        "    def train_step(self, train_loader, epoch):\n",
        "\n",
        "        total_loss = 0\n",
        "        n = len(train_loader)\n",
        "\n",
        "        with tqdm(total=n) as pbar:\n",
        "            for step, batch in enumerate(train_loader):\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                inputs, targets = batch\n",
        "\n",
        "                inputs = inputs.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                input_ids = inputs.input_ids\n",
        "                target_ids = targets.input_ids\n",
        "\n",
        "                output = self.model(input_ids=input_ids, decoder_input_ids=target_ids)\n",
        "\n",
        "                target_ids = self.model.extend_labels(target_ids, self.pad_token_idx)\n",
        "\n",
        "                logits = output.logits\n",
        "\n",
        "                logits_dim = logits.shape[-1]\n",
        "\n",
        "                logits = logits[1:].view(-1, logits_dim)\n",
        "                target_ids = target_ids[1:].reshape(-1)\n",
        "\n",
        "                loss = self.criterion(logits, target_ids)\n",
        "                \n",
        "                loss.backward()\n",
        "\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                if (step+1) % 100 == 0:\n",
        "                    print(f\"\\nEpoch {epoch}, samples {step+1}/{n} train loss: {total_loss/(step+1)}\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "                \n",
        "        avg_loss = total_loss / n\n",
        "\n",
        "        return avg_loss\n",
        "            \n",
        "    \n",
        "    def val_step(self, val_loader, epoch):\n",
        "\n",
        "        total_loss = 0\n",
        "        n = len(val_loader)\n",
        "\n",
        "        with tqdm(total=n) as pbar:\n",
        "            for step, batch in enumerate(val_loader):\n",
        "\n",
        "                inputs, targets = batch\n",
        "\n",
        "                inputs = inputs.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                input_ids = inputs.input_ids\n",
        "                target_ids = targets.input_ids\n",
        "\n",
        "                output = self.model(input_ids=input_ids, decoder_input_ids=target_ids)\n",
        "\n",
        "                target_ids = self.model.extend_labels(target_ids, self.pad_token_idx)\n",
        "\n",
        "                logits = output.logits\n",
        "\n",
        "                logits_dim = logits.shape[-1]\n",
        "\n",
        "                logits = logits[1:].view(-1, logits_dim)\n",
        "                target_ids = target_ids[1:].reshape(-1)\n",
        "\n",
        "                loss = self.criterion(logits, target_ids)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                if (step+1) % 100 == 0:\n",
        "                    print(f\"\\nEpoch {epoch}, samples {step+1}/{n} train loss: {total_loss/(step+1)}\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "        avg_loss = total_loss / n\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "\n",
        "    \n",
        "    def test_step(self, test_loader):\n",
        "\n",
        "        self.model.load_state_dict(torch.load(f\"{CHECKPOINT_DIR}/model_{self.model_name}_{self.best_epoch}_checkpoint.pt\"))\n",
        "        \n",
        "        total_loss = 0\n",
        "        n = len(test_loader)\n",
        "\n",
        "        with tqdm(total=n) as pbar:\n",
        "            for step, batch in enumerate(test_loader):\n",
        "                \n",
        "                inputs, targets = batch\n",
        "\n",
        "                inputs = inputs.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                input_ids = inputs.input_ids\n",
        "                target_ids = targets.input_ids\n",
        "\n",
        "                output = self.model(input_ids=input_ids, decoder_input_ids=target_ids)\n",
        "\n",
        "                target_ids = self.model.extend_labels(target_ids, self.pad_token_idx)\n",
        "\n",
        "                logits = output.logits\n",
        "\n",
        "                logits_dim = logits.shape[-1]\n",
        "\n",
        "                logits = logits[1:].view(-1, logits_dim)\n",
        "                target_ids = target_ids[1:].reshape(-1)\n",
        "\n",
        "                loss = self.criterion(logits, target_ids)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "\n",
        "        avg_loss = total_loss / n\n",
        "\n",
        "        return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xihgs1m6LPpR",
        "outputId": "fa3847d4-c534-4fe7-eb23-a970318f90f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 37824 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "model = T5PromptTuning.from_pretrained(\n",
        "    \"t5-small\",\n",
        "    encoder_soft_prompt_path = None,\n",
        "    decoder_soft_prompt_path = None,\n",
        "    encoder_n_tokens = 20,\n",
        "    decoder_n_tokens = 40,\n",
        "    encoder_hidden_dim=32,\n",
        "    decoder_hidden_dim=32\n",
        ")\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PId4BhAtzZBe"
      },
      "source": [
        "## Define and train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "1fTM0U_-zZMo",
        "outputId": "22a52793-a4f0-4d10-eb42-69b1ca596f59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training epoch 1/1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 100/31751 [00:20<1:45:02,  5.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1, samples 100/31751 train loss: 7.0314547729492185\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 201/31751 [00:40<1:39:08,  5.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1, samples 200/31751 train loss: 6.892015600204468\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 275/31751 [00:54<1:43:34,  5.06it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-402b90cbca66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_fun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-dfb3548b1fe3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, generate_fun)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-dfb3548b1fe3>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(self, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training epoch {epoch}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation epoch {epoch}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-fc99c236d924>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, train_loader, epoch)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "config = {\n",
        "    \"src_max_length\": 183,\n",
        "    \"dst_max_length\": 208,\n",
        "    \"src_vocab_size\": 31102,\n",
        "    \"dst_vocab_size\": 28996,\n",
        "    \"enc_hidden_dim\": 8,\n",
        "    \"dec_hidden_dim\": 8,\n",
        "    \"max_epochs\": 1,\n",
        "    \"batch_size\": 8,\n",
        "    \"seed\": 7,\n",
        "    \"device\": device\n",
        "}\n",
        "\n",
        "\n",
        "src_tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-italian-cased\")\n",
        "dst_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "model = T5PromptTuning.from_pretrained(\n",
        "    \"t5-small\",\n",
        "    encoder_soft_prompt_path = None,\n",
        "    decoder_soft_prompt_path = None,\n",
        "    encoder_n_tokens = 40,\n",
        "    decoder_n_tokens = 40,\n",
        "    encoder_hidden_dim=64,\n",
        "    decoder_hidden_dim=64,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "trainer = PromptTuningTrainer(model, src_tokenizer, dst_tokenizer, config)\n",
        "\n",
        "generate_fun = lambda x: model.generate(\n",
        "    input_ids=x, \n",
        "    decoder_input_ids=torch.zeros([1,1]).long().to(config[\"device\"]), \n",
        "    max_length=200,\n",
        "    num_beams=5,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "trainer.train(generate_fun)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
